[{"title":"A descriptive framework for the field of deep learning applications in medical images","date":"2022-06-08T16:00:00.000Z","url":"/2022/06/09/DeepLearningMedical/","tags":[["Deep_Learning","/tags/Deep-Learning/"]],"categories":[["Artificial Intelligence","/categories/Artificial-Intelligence/"]],"content":" How to define the “descriptive framework”? The work of this paper shows comprehensive analysis of historical papers, but this work is still a little confusing to me, why it could be called framework? It is noticed that GAN is also used in the medical images analsis. Actually, in the medical industry, personal health senario is quite complex. Is it meaningful trying to train generative data by algorithm before we really understand the actual medical situation? 0.1 Deep Learning Supervised Learning Weakly Supervised Learning Unsupervised learning Supervised Learning Supervised learning represents a learning method that uses data with manual annotation labels to train the network. For classification,image-level or patch-level labels are required. . In the supervised setting, given datasets D = {(x1, y1), (x2, y2), . . . , (xN , yN )}, the corresponding loss aims to minimize the distance between the predicted value y′ and the ground truth y, as Eq. (1). Weakly Supervised Learning Weakly supervised learning mainly consists of incomplete su\u0002pervision, inexact supervision and inaccurate supervision [104], where incomplete supervision is more common in the medical field. The datasets in incomplete supervision can be denoted as D = {(x1, y1), (x2, y2), . . . , (xl, yl), xl+1, . . . , xN }, where only a small subset of labeled data is available. Unsupervised learning Unsupervised learning attempts to train a model with unlabeled data. The Generative unsupervised model is represented by Autoencoder (AE), Sparse Autoencoder (SAE) and Generative Adversarial Network (GAN), which focuses on pixel-level reconstruction error. However, pixel-level generation is computationally expensive. Recently,self-supervised learning (SSL) has become more and more popular. Contrastive learning (CL) is a type of SSL, which has achieved great progress in computer vision, natural language processing and other fields. It hopes to distinguish different inputs in the feature space. The loss function is shown in formula (2) [108], aiming to measure the similarity between two features. 0.2 Convolutional neural network Classification architectures Object detection architectures Segmentation architectures At present, the Convolutional Neural Network (CNN) is the most popular deep neural network for computer vision. The original deep network architecture used for image classification, object detection and segmentation was born based on CNN in supervised learning scenarios. Almost all representative articles are based on CNN. The convolution layer consists of several convolution kernels (filters) and each can be expressed as a linear function in Eq. Classification Architecture For the classification task, there is only one object of a single category in an image/patch. The main process of classification is to feed images into a deep learning model, where the image-level/patch-level labels are required, and finally output the category or its corresponding probability of this image. Object detection architectures Medical image detection is an object detection task in deep learning, where there are multiple objects and categories in one image. Besides classifying images, the typic feature of object detection is locating the position or coordinates of an object with a bounding box. Segmentation architectures Deep learning-based segmentation can be divided into ordinary segmentation, semantic segmentation, and instance segmentation. In the medical field, semantic segmentation is the most common. Compared with image classification and object detection, segmentation is more accurate and more complicated. It outputs pixel-level labels instead of image-level labels. Besides the bounding box, it can also output the exact boundary of the object (Fig. 7). 0.3 Recurrent neural network Recurrent Neural Network (RNN) is a neural network designed to process sequence data. Finally, the output layer is only activated based on the hiddenstate of the last layer, that is, the lth layer, which can be shownin Eq. (7). 0.4 Generative adversarial network Generative Adversarial Network (GAN) introduced by Good fellow et al. [119] is one of the most common architectures in unsupervised learning. It consists of a generator (G) and a discriminator (D). The two of them are playing against each other. The G is used to capture the distribution of the input samples and generate fake images from noise prior as close as possible to the real images, so as to deceive the D. In contrast, the D needs to distinguish whether the sample comes from real data or generated data as much as possible. The value function V(D, G) is shown in Eq. (6). $$V(G,D) =\\mathop{min}\\limits_{G}\\enspace\\mathop{max}\\limits_{D} \\mathbb{E}{x\\sim p_data}(x)[logD(x)]+\\mathbb{E}{z\\sim p_z}(z)[log(1-D(G(z)))]$$ 0.1 Applications of classification Pathologic image X-ray OCT and fundus image Endoscopic image Dermoscopic image EEG / ECG Other images This section summarizes the application of 77 representative articles for the field of deep learning in medical image classification,detection, segmentation and generation. Pathologic image For cancer, pathological diagnosis is a gold standard in clinical practice. The conventional diagnostic way is to visually inspect histopathology images by experienced pathologists. X-ray Mammography is one of the most important modes in X-ray images and the main diagnostic tool for breast cancer. OCT and fundus image Almost all articles that we surveyed on OCT and fundus images were about classification. OCT and fundus images are diagnostic tools specifically used to examine retinal abnormalities. Diabetic retinopathy (DR), glaucomatous optic neuropathy (GON) and agerelated macular degeneration (AMD) are common retinal diseases. Endoscopic image Endoscopy produces video and it always takes a long timeto analyze, even for professional endoscopists. There have beenNarrow band imaging (NBI) — based deep learning studies aboutdistinguishing normal and abnormal colorectal polyps [44,45].NBI is one of the most important forms of endoscopy, with threemodes of far focus, near focus and normal focus. Dermoscopic image Dermoscopic image-based deep learning models are oftenused to diagnose melanoma. There are two related studies fromHeidelberg University in Germany. EEG / ECG EEG and ECG are biological signals and usually seen in clinicaldiagnosis with special image modalities. 0.2 Applications of detection Endoscopic image X-ray and CT Other images Endoscopic image Detection is more challenging in endoscopy than in otherordinary images. For colonoscopy, it consists of thousands offrames, probably 20–35 frames per second. The polyp can bedetected in only a few frames [68]. Therefore, real-time detectionin endoscopy is of great significance. X-ray and CT Both chest radiograph and CT are widely used in clinical diagnosis. Chest radiograph requires less dose but has poorer image quality. In contrast, CT has a wider range of applicationsand higher image quality. However, even low dose CT scansrequire 50–100 times radiation dose than single-view chest radiograph [74] 03 Applications of segmentation MRI CT Other images MRI MRI has a superior high-resolution soft-tissue contrast andcan display lesions from multiple views (horizontal, coronal, andsagittal, for example). Therefore, it has been most widely used inthe diagnosis of pelvic, heart, brain, knee joints and other parts. CT Roy et al. [81] developed a framework based on few-shortlearning and ‘squeeze &amp; excite’ blocks to realize CT volumetricimage segmentation. Chen et al. [94] achieved a bidirectionalcross-modality adaptation between MRI and CT images via imageand feature alignment. 04 pplications for image generation This section is highly relevant to PET, CT and MR, and GAN iswidely welcomed here. In fact, considerable articles also belongto the segmentation task. Here we set them as a separate imagegeneration section, including image generation in a narrow sense,image translation, image denoising, image reconstruction, imageattenuation correction, and so on. The analysis of medical images is an important tool for clinicaldiagnosis. In this survey, 2068 articles published on PubMedon and before June 26, 2020 about deep learning in medicalimages were reviewed. The number of articles has experiencedexponential growth since 2017. The main contribution in this survey is providing a descriptiveframework for reviewing and using LDA as a quick analysis toolfor literature review. "},{"title":"Incomplete-view oriented kernel learning method with generalization error bound","date":"2022-06-07T16:00:00.000Z","url":"/2022/06/08/KernelLearning/","tags":[["Kernel_Learning","/tags/Kernel-Learning/"]],"categories":[["Artificial Intelligence","/categories/Artificial-Intelligence/"]],"content":" What’s the difference between Multimodal Machine Learning and Multi-view Learning? How to define the specific concept? Are the two terms refer to the same thing? Is the the RPSVM-2V solution just linear Integration of RSVM and PSVM-2V? According to the data completeness, jump to the certain branch by “IF”? 0.1 KL(Kernel Methods) Support Vector Machines (SVM) Radial Basis Function (RBF) Linear Discriminate Analysis (LDA) Kernels or kernel methods (also called Kernel functions) are sets of different types of algorithms that are being used for pattern analysis. They are used to solve a non-linear problem by using a linear classifier. Kernels Methods are employed in SVM (Support Vector Machines) which are used in classification and regression problems. Support Vector Machines (SVM) In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. 最优参数的SVM决策函数： $$f(x)=\\text{sgn}(w^{T}x+b^)=\\text{sgn}(\\sum_{n=1}^N \\lambda_n^y^n(x^n)^Tx+b^)$$ 在一个变换后的特征空间中，SVM的决策函数： $$f(x) = sgn(w^T\\phi(x)+b^*) = sgn(\\sum_{n=1}^N \\lambda_n^y^nK(x^n,x)x+b^)$$ 其中核函数（Kernel）为： Radial Basis Function (RBF) A radial basis function (RBF) is a real-valued function φ whose value depends only on the distance between the input and some fixed point, either the origin, so that φ(𝐱)=φ(|𝐱|), or some other fixed point 𝐜, called a center, so that φ(𝐱)=φ(|𝐱-𝐜|). Any function φ that satisfies the property φ(𝐱)=φ(|𝐱|) is a radial function. The distance is usually Euclidean distance, although other metrics are sometimes used. Linear Discriminate Analysis (LDA) Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher’s linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification. 0.2 MKL(Multiple Kernel Learning) Supervised learning Fixed rules approaches Heuristic approaches Optimization approaches Bayesian approaches Boosting approaches Semi-Supervised learning UnSupervised learning Multiple kernel learning refers to a set of machine learning methods that use a predefined set of kernels and learn an optimal linear or non-linear combination of kernels as part of the algorithm. Reasons to use multiple kernel learning include a) the ability to select for an optimal kernel and parameters from a larger set of kernels, reducing bias due to kernel selection while allowing for more automated machine learning methods, and b combining data from different sources that have different notions of similarity and thus require different kernels. Instead of creating a new kernel, multiple kernel algorithms can be used to combine kernels already established for each individual data source. Multiple kernel learning (MKL) algorithms aim to find the best convex combination of a set of kernels to form the best classifier. Many algorithms have been presented in recent years and they form two classes. Supervised learning Fixed rules approaches Heuristic approaches These algorithms use a combination function that is parameterized. we can define Other approaches use a definition of kernel similarity, such as Optimization approaches These approaches solve an optimization problem to determine parameters for the kernel combination function. This has been done with similarity measures and structural risk minimization approaches. For similarity measures such as the one defined above, the problem can be formulated as follows:[9] $$\\max_{\\beta,\\operatorname {tr}(K’{tra})=1,K’\\geq 0}A(K’{tra},YY^{T}).$$ Bayesian approaches Bayesian approaches put priors on the kernel parameters and learn the parameter values from the priors and the base algorithm. For example, the decision function can be written as Boosting approaches Boosting approaches add new kernels iteratively until some stopping criteria that is a function of performance is reached. An example of this is the MARK model developed by Bennett et al. (2002) [14] Semi-Supervised learning Let be the labeled data, and let be the set of unlabeled data. Then, we can write the decision function as follows. The problem can be written as where is the loss function (weighted negative log-likelihood in this case), is the regularization parameter (Group LASSO in this case), and is the conditional expectation consensus (CEC) penalty on unlabeled data. The CEC penalty is defined as follows. Let the marginal kernel density for all the data be where (the kernel distance between the labeled data and all of the labeled and unlabeled data) and is a non-negative random vector with a 2-norm of 1. The value of is the number of times each kernel is projected. Expectation regularization is then performed on the MKD, resulting in a reference expectation and model expectation . Then, we define where is the Kullback-Leibler divergence. The combined minimization problem is optimized using a modified block gradient descent algorithm. For more information, see Wang et al.[15] UnSupervised learning Combining these terms, we can write the minimization problem as follows. where . One formulation of this is defined as follows. Let be a matrix such that means that and are neighbors. Then, . Note that these groups must be learned as well. Zhuang et al. solve this problem by an alternating minimization method for and the group . For more information, see Zhuang et al.[16] 0.3 MVL(Multi-view Learning) Data in reality often exhibit in multi-modal forms, called multi-view data. Each view distributed in distinct feature spacesdescribes different attributes of the same object with high dimension, strong heterogeneity and rich description. The performance of multi-view learning models compares more favorably than that of single-view learning models. The existing MVLapproaches either comply with the consensus principle or the complementary principle. Here we mainly review SVM-basedMVL classification methods. 1.1 Multi-view data and learning The simplest way is to train the model on the concatenated view. However, this approach ignores the correlation and interaction among views, which may cause overfitting and dimension disasters [3,4]. In this case, multi-view learning (MVL) has emerged and achieved great success in classification [5], clustering[6,7], feature selection [8,9], etc. With the help of different kinds sensor equipment, data tracking from different aspects is much easier. In most of the cases, multi modal data gives a better view of a certain event, characterising the specific object in comprehensive way, by providing more information. 1.2 Incomplete multi-view data In real-world applications, we often confront an obstacle that only partial views are available, due to the difficulties of highcost, equipment failure, and so on [13]. Thus, learning with multiple incomplete views is a challenging yet valuable work. Unfortunately, the aforementioned strategies may either lose important information or introduce some errors, especially for the case where the amount of incomplete multi-view data is particularly large. In contrast, learning from massive complete-view data without view-missing is also complicated and time-consuming. reduced support vector machine (RSVM) multi-view privileged support vector machine (PSVM-2V) mix solution RPSVM-2V(Integration with RSVM and PSVM-2V) RSVM The RSVM can be expressed as follows: PSVM-2V To begin with, we can directly reformulate PSVM-2V (1) for incomplete-view learning, as shown below. $$s.t. \\enspace \\enspace \\enspace \\enspace |[[&lt;W_{A_1},\\Phi_{A_1}&gt;]]l^r-[[&lt;w_{A_2}&gt;,\\Phi{A_2}]]_l^r |\\le \\eta+e\\epsilon,$$ $$[[\\xi^{A_2}]]l^r \\ge [[D_1&lt;W_{A_1,\\Phi_{A_1}}&gt;]]l^r,\\xi^{A_1},\\xi^{A_2},\\eta \\ge 0, W{A_k} = \\sum{n=1}^N v_{A_k}^{j} \\phi_{k} (X_{j}^{A_k}) $$.Then problem (3) can be rewritten as follows. $$s.t.\\enspace \\enspace \\enspace \\enspace|[[K_{A_1}(A_1,A_1’)v_{A_1}]]l^r-[[K{A_2}(A_2,\\overline{A_2’}v_{A_2})]]| \\le \\eta + e \\epsilon$$ $$[[\\xi^{A_1}]]l^r \\ge [[D_2(K{A2,A_2’}v_{A_2})]]_l^r$$ $$[[\\xi^{A_2}]]l^r \\ge [[D_1(K{A1,A_1’}v_{A_1})]]_l^r$$ RPSVM-2V(*****) Formally, RPSVM-2V can be built as follows. $$[[\\xi^{A_1}]]l^r \\ge [[D_2 \\widetilde{K{A_2}}(A_2,\\overline{A_2’})\\widetilde{v_{A_2}}]]_l^r$$ $$[[\\xi^{A_2}]]l^r \\ge [[D_1 \\widetilde{K{A_1}}(A_1,\\overline{A_1’})\\widetilde{v_{A_1}}]]_l^r\\xi^{A_1},\\xi^{A_2},\\eta \\ge 0.$$ generalization error bound Generalized performance parameter study In this section, proof and algorithm are stated in detial for the final expriment output. In this paper, we propose a new model RPSVM-2V by integrating RSVM with PSVM-2V in incomplete- and complete-viewscenarios. RPSVM-2V can not only fully leverage all available information for the incomplete views case, but also achievecomparable performance with less computation and storage cost in the complete-view setting. "},{"title":"Latex for Hexo Markdown Engine","date":"2020-05-12T16:00:00.000Z","url":"/2020/05/13/LatexLearning/","tags":[["Latex_Learning","/tags/Latex-Learning/"]],"categories":[["Software Engineering","/categories/Software-Engineering/"]],"content":"本文主要对Latex在Markdown中的渲染语法进行学习和熟悉 第一部分:希腊字母基础表达 实际渲染效果如下 正常和变体 正常和变体 首字母大写，对应希腊字母也大写 第二部分：公式上下标处理 用大括号包裹多项式下标是斜体还是直立体的和的区别和对后面的作用域区别注意里面是用大括号而不是小括号作为作用域的常量建议用直立体的字母 第三部分：分式和根式的表达 分式语法：分子分母嵌套分式：分子中的较小，改为调整格式根式次方根 第四部分：数学运算符 第五部分：特殊运算符和函数 加入，则的角标在的下方 第六部分：大型运算符 这里上下限均现实在求和求积的右侧，若显示在上下侧需要加入limits限定如下此处曲面积分和球面积分无法正确渲染，问题待解决但在严谨场合中的为直立体，且与被积函数应该拉开一段小间隔 第七部分：标注符号、箭头、定界符 标注符号：上面一个右箭头上面一个横线箭头单箭头双箭头括号和定界符：大括号的左右括号要加转义高度自适应的括号：为了自适应竖线，构造了虚拟的左括号： 第八部分：多行公式和矩阵 其他 "},{"title":"Tiny JavaScript Compiler","date":"2018-10-01T15:00:00.000Z","url":"/2018/10/01/2018-10-24-Tiny-Compiler/","tags":[["Compilers","/tags/Compilers/"],["Lexical Analysis","/tags/Lexical-Analysis/"],["Syntax Analysis","/tags/Syntax-Analysis/"],["Computer Science","/tags/Computer-Science/"]],"categories":[["Compilers","/categories/Compilers/"],["Lexical Analysis","/categories/Compilers/Lexical-Analysis/"],["Syntax Analysis","/categories/Compilers/Lexical-Analysis/Syntax-Analysis/"],["Computer Science","/categories/Compilers/Lexical-Analysis/Syntax-Analysis/Computer-Science/"]],"content":" I already wrote a couple of essays related to the development of programming languages that I was extremely excited about! For instance, in “Static Code Analysis of Angular 2 and TypeScript Projects“[1] I explored the basics of the front end of the compilers, explaining the phases of lexical analysis, syntax analysis and abstract-syntax trees. Recently I published “Developing Statically Typed Programming Language“[2]. This post shown a simple, statically typed, functional programming language inspired by lambda calculus. There I outsourced the front end part of the compiler development to a parser generator and focused on the back end in the faces of a module for type checking and one for code generation. Why do I need this?You might be now wondering “Why would I need to know how to develop a compiler?“. There are a few important reasons: You will get a better understanding of how the programming languages you’re using work. This will allow you to develop more efficient programs with them. Often, you’ll have to reuse pieces of the modules described below for different purposes (for instance, parsing of configuration files, parsing of network messages, etc.). Creating a DSL. Creating a Domain Specific Language in your project could be quite handy in order to simplify tasks which otherwise take a lot of time to solve with a general purpose programming language. What are we going to cover?In this blog post we’ll cover the basics from end-to-end! We’ll develop an extremely simple compiler on 25 lines of JavaScript! Our compiler will have: Module for lexical analysis Module for syntax analysis The parser will be based on an EBNF grammar We will develop the parser by using a recursive descent parsing algorithm Code generator The language that we’re going to explore is not particularly useful for developing meaningful software programs but it can be easily extended to one. The entire implementation can be found at my GitHub profile[3]. Introducing a Simple Prefix LanguageHere’s how a sample expression in our language is going to look like: By the end of this article we’ll be able to transpile these expressions to JavaScript by going through phases typical for any compiler! For simplicity, there are a few rules we need to follow: We have only the functions: mul, sub, sum, div. Each individual string token is surrounded by whitespace. We support only natural numbers. In order to explore the semantics behind the expression above lets define a few JavaScript functions: mul accepts multiple operands, passed with the spread operator. The function just multiplies all of them, so for instance mul(2, 3, 4) === 24. sub respectively subtracts the passed arguments and sum sums them. The expression above can be translated to the following JavaScript expression: or… Now, after we have understanding of the semantics, lets start with the front end of the compiler! Note: Similar prefix expressions can be simply evaluated with a stack-based algorithm, however, in this case we’ll focus on concepts rather than implementation. Developing the Compiler’s Front EndThe front end of any compiler usually has the modules for Lexical Analysis[4] and Syntax Analysis[5]. In this section we’ll build both modules in a few lines of JavaScript! Developing a Lexical AnalyzerThe phase of lexical analysis is responsible for dividing the input string (or stream of characters) of the program into smaller pieces called tokens. The tokens usually carries information about their type (if they are numbers, operators, keywords, identifiers, etc), the substring of the program they represent and their position in the program. The position is usually used for reporting user friendly errors in case of invalid syntactical constructs. For instance, if we have the JavaScript program: A sample lexical analyzer for JavaScript will produce the output: We’ll keep our lexical analyzer as simple as possible. In fact, we’ll implement it on a single line of JavaScript: Here we split the string by a single space, we map the produced substrings to their trimmed version and filter the empty strings. Invoking the lexer with an expression will produce an array of strings: This is completely enough for our purpose! Now lets go to the phase of syntax analysis! Developing a ParserThe syntax analyzer (often know as parser) is the module of a compiler which out of a list (or stream) of tokens produces an Abstract Syntax Tree[6] (or in short an AST). Along the process, the syntax analyzer may also produce syntax errors in case of invalid programs. Usually, the parser is implemented based on a grammar[7]. Here’s the grammar of our language: This basically means that we have digits which composed together can form numbers (num). We have 4 operations and an expression can be either a number, or operation, followed by one or more expressions. We’ll refer to the individual definitions in the grammar (for instance to num and op) as rules. This is the so called EBNF grammar[6]. Look at the grammar for a bit, try to understand it, and after that completely forget about it! We’ll come back to the grammar after we explain the parser and you’ll see how everything connects together! As we mentioned, a parser is a tool which turns a list of tokens to an AST. For instance, for our expression: The parser will produce the following AST, based on the grammar above: Lets explore the algorithm for this! The bad news is that there are a lot of things going on. The good news is that this is the most complicated part of the compiler! Lets divide the code into parts and look into each one step by step. Node Types First we define the different node types that we are going to have in the AST. We’ll have only numbers and operations. For example, the number node 42 will look like this: The operator sum, applied to 2, 3, 4 will look like this: That’s how simple is that! The ParserAfter we declare the node types, we define a function called parse which accepts a single argument called tokens. Inside of it we define five more functions: peek - returns the element of tokens associated with the current value of the c local variable. consume - returns the element of tokens associated with the current value of the c local variable and increments c. parseNum - gets the current token (i.e. invokes peek()), parses it to a natural number and returns a new number token. parseOp - we’ll explore in a little bit. parseExpr - checks if the current token matches the regular expression /\\d/ (i.e. is a number) and invokes parseNum if the match was successful, otherwise returns parseOp. Parsing OperationsThe parseOp is maybe the most complicated function from the parser above. That’s the case because of the loop and the indirect recursion that we have. Here’s its definition one more time in order to explain it line by line: Since parseOp has been invoked by parseExpr when the value of peek() is not a number we know that it is an operator so we create a new operation node. Note that we don’t perform any further validation, however, in a real-world programming language we’d want to do that and eventually throw a syntax error in case of unexpected token. Anyhow, in the node declaration we set the list of “sub-expressions” to be the empty list (i.e. []), the operation name to the value of consume() and the type of the node to Op. Later, while we don’t reach the end of the program, we loop over all tokens by pushing the currently parsed expression to the list of “sub-expressions` of the given node. Finally, we return the node. Keep in mind that while (peek()) node.expr.push(parseExpr()); performs an indirect recursion. In case we have the expression: This will First, invoke parseExpr, which will find that the current token (i.e. tokens[0]) is not a number (it’s sum) so it’ll invoke parseOp. After that parseOp will create the operation node and because of the consume() call, increment the value of c. Next parseOp will iterate over the tokens, and for tokens[c], where c now equals 1 will invoke parseExpr. parseExpr will find that the current token is not a number so it’ll invoke parseOp. parseOp will create another operation node and increment c and will start looping over the remaining tokens again. parseOp will invoke parseExpr where c will now equal 2. Since tokens[2] === \"2\", parseExpr will invoke parseNum which will create a number node, incrementing the c variable. parseNum will return the number node and it will be pushed into the expr array of the last operation node produced by the latest parseOp invocation. The last parseOp invocation will return the operation node since peek() will return undefined (parseNum has incremented c to 3 and tokens[3] === undefined). The node returned by the last invocation of parseOp will be returned to the outermost invocation of parseOp which will return its operation node as well. Finally, parseExpr will return the root operation node. The produced AST will look like: …and that’s it! The final step is to traverse this tree and produce JavaScript! Recursive Descent ParsingNow lets see how are the individual functions related to the grammar we defined above and see why having a grammar makes sense in general. Lets take a look at the rules in the EBNF grammar: Do they now may make a bit more sense? expr looks very much like parseExpr, where we parse either a number or an operation. Similarly, op expr+ looks very much like parseOp and num like parseNum. In fact, very often parsers are generated directly from the grammars since there’s a direct connection between both with the recursive descent parsing algorithm[8]. And in fact, we just developed a simple recursive descent parser! Our parser was quite simple (well, we have only 4 production rules in the grammar) but you can imagine how complex the parser of a real-life programming language is. It’s extremely convenient to develop the grammar of a language before writing the actual parser in order to observe a simplified model of it. The parser contains a lot of details (for instance a lot of syntax constructs of the language you’re developing it with), in contrast to the grammar which is extremely simplified and minimalistic. Developing the TranspilerIn this part of the post we’ll traverse the AST of the language and produce JavaScript. The entire transpiler is on 7 lines of JavaScript (literally!): Lets explore the implementation line by line. First we define a function called transpile. It accepts as argument the AST produced by the parser. After that in the opMap we define the mapping between arithmetic operations and the operators in the language. Basically, sum maps to +, mul to *, etc. As next step, we define the function transpileNode which accepts an AST node. If the node is a number, we invoke the transpileNum function with the given node, otherwise, we invoke transpileOp. Finally, we define the two functions for transpilation of the individual nodes: transpileNum - translates a number to a JavaScript number (simply by returning it). transpileOp - translates an operation to a JavaScript arithmetic operation. Notice that we have indirect recursion here (transpileOp -&gt; transpileNode -&gt; transpileOp). For each operation node, we want to transpile its sub-expressions first. We do that by invoking the transpileNode function. On the last line of transpile‘s body, we return the result of transpileNode applied to the root of the tree. Wiring Everything TogetherHere’s how we can wire everything together: We invoke lex(program), which produces the list of tokens, after that we pass the tokens to the parse function, which produces the AST and finally, we transpile the AST to JavaScript! ConclusionThis article explained in details the development of a very simple compiler (or transpiler) of a language with prefix expressions to JavaScript. Although this was explanation of only the very basics of the compiler development we were able to cover few very important concepts: Lexical analysis Syntax analysis Source code generation EBNF grammars Recursive Descent Parsing If you’re interested in further reading, I’d recommend: Developing Statically Typed Programming Language Let’s Build A Simple Interpreter Compilers: Principles, Techniques, and Tools (2nd Edition) 2nd Edition Types and Programming Languages References Static Code Analysis of Angular 2 and TypeScript Projects - . Developing Statically Typed Programming Language -  Tiny Compiler -  Lexical Analysis -  Syntax Analysis -  Abstract Syntax Tree -  EBNF grammar -  Recursive Descent Parsing -  "},{"title":"Semantic Knowledge and its Application","date":"2017-06-26T15:00:00.000Z","url":"/2017/06/26/2017-06-27-Semantic-knowledge-and-application/","tags":[["Computational Linguistics Semantics","/tags/Computational-Linguistics-Semantics/"]],"categories":[["Computational-Linguistics","/categories/Computational-Linguistics/"]],"content":"语义学（Semantics），也作“语意学”，是一个涉及到语言学、逻辑学、计算机科学、自然语言处理、认知科学、心理学等诸多领域的一个术语。 虽然各个学科之间对语义学的研究有一定的共同性，但是具体的研究方法和内容大相径庭。语义学的研究对象是自然语言的意义，这里的自然语言可以是词，短语（词组），句子，篇章等等不同级别的语言单位。但是各个领域里对语言的意义的研究目的不同： 语言学的语义学研究目的在于找出语义表达的规律性、内在解释、不同语言在语义表达方面的个性以及共性； 逻辑学的语义学是对一个逻辑系统的解释，着眼点在于真值条件，不直接涉及自然语言； 计算机科学相关的语义学研究在于机器对自然语言的理解； 认知科学对语义学的研究在于人脑对语言单位的意义的存储及理解的模式。 计算角度下的语义语义的通俗理解首先，我们从正常的通俗概念上来理解语义是什么。在日常的对话和各种场合的表述中，通常一句话表达的信息和含义，我们能够用不同的方式进行体现。在我们还是孩提的时候，老师就曾经教过我们采用不同的方式对同一个句子进行改写以表达同样的信息。例如： “张三打了李四” ——&gt; “李四被张三打了” 在这样的两个句子中，句子本身的形式不同，但是其信息中表示的不同实体之间的关系是一致的。这里在语义的层面上，同样都是：张三 “打” 李四。但是，很多时候我们并不会进行深入的思考，这种对同样语义进行的不同形式的展现，究竟意味着什么。尤其是在复杂场景下，更是这种情况。某一种表述和原先的表述可能信息表述的信息相同，但是在其特定的环境下，究竟应该选择哪一种方式，很多的时候，我们并不能精确的描述出来。 符号学上的意义我们从纯符号学的角度来看意义，可能并没有什么意义————意义就是符号变换的游戏，纯粹从符号世界的角度来理解：“A的意思是B” or “A意味着B”。然而，所谓知道一个符号串的意义，包含两层意思： 你可以把一个符号串A变换为符号串B； 原则上，关于这个符号串A的变换游戏没有止尽; 在英文中有一个词，用来表示同样意思之间的形式转换：Paraphrase，下面我们看一下Collins字典中对这个英文词的解释。 If you paraphrase someone or paraphrase somethingthat they have said or written, you express what theyhave said or written in a different way. —— Collins COBUILD Dictionary 在上述的特点下，自然语言系统中因为形式与意义的非一一对应性，导致了在计算语言学中对其语言材料的处理是一个极其困难的过程。 一个形式 —— 多个意义:自然语言理解必须面对的问题 一个意义 —— 多个形式:自然语言生成必须面对的问题 这两种不对应，如果要进行具体的分类可以基本上涵盖以下的不同类型： 1.一个形式多重意义的情况： 概念多义:如“编辑”这个词同时具有不同的词性，当然也就用法不同。 指称多义:如“张三把他出卖了。”和“张三把他的朋友出卖了。”两句中他到底指的是谁？ 泛指与特指:略 量化词组:略 常见歧义结构：常见的语言中各种特例，如牛奶饼干，晒太阳等。 隐喻:如”董永这回算是背上口大黑锅了.” 2.一个意义多种形式的情况： 同义词:如“衣服 — 衣裳”等。 同义结构:如“去看看”和“看看去” 在自然语言中上述的这类情况非常多，具体的语言材料零碎，且很多时候这些语言信息和不同的民族文化又有很大的关联。在面对这些问题的时候，就需要耗费一番功夫。 语义知识简述语义知识从词义聚合和词义组合两种基本基本的特性上可以大致按照这两个方向进行划分。 关于词义聚合关系的知识： 1.语义特征（义素）分析 2.语义关系分析：语义分类（语义场）—— 上、下位关系同义关系；反义关系；整体 - 部分关系 … 上述两种不同的分析，如果深入进去会有更详细的阐述，以后有机会的话，深入专题。这里只做简单的整理分类，以便从宏观的角度看整体的方向。 关于词义组合关系的知识: 理想目标：描述任意词语之间的组配关系 配价理论：典型的配价关系：动词 － 名词之间的组配关系可分为三个层次来描述这种组配关系知识: 1.论元个数 —— 价数 2.论元类型 —— 论旨角色 3.对论旨角色的选择限制 在词义组合的方面，有着更为详细的处理，细节很多，这里也不做深入。 语义知识的应用语义知识跟句法知识的差别主要在于知识颗粒度的不同，从某种意义上说，语义知识就是细化了的句法知识。 语义知识的应用： 帮助判断一个形式的合语法性，帮助做句法结构分析 帮助做形式变换：得到同义表达式，进行推理，等等 从本质上来说，语义知识本身也是一种形式化的数据集合。在不同的情况下，语义本身的原则对语言材料做出一定的指导作用。上面所说的两种应用，第一种是在语义知识的基础上用来对实际的语言特征进行筛选。第二种，则是在变换规则的基础上，对某种符合语义知识的语言材料进行映射的过程。 详细的例子，这里也不做过分的细说。在此过程中，会产生很多的问题，接下来在理论工具的发展下，来一步步了解：对这些具体的问题，怎样用计算的方式进行解决？这是一个艰苦的过程。 讲义最后的文献 冯志伟等译（2005）《自然语言处理综论》第14，15，16章。 詹卫东，2001，《确立语义范畴的原则及语义范畴的相对性》，载《世界汉语教育》2001年第2期。 徐烈炯，1995，《语义学》，语文出版社 Geoffrey Leech, 1983, Semantics: The Study * of Meaning, Penguin Books. 蒋严，潘海华，1998，《形式语义学引论》，中国社会科学出版社 Shalom Lappin, ed., 1996, The Handbook of Contemporary Semantic Theory, Oxford:Blackwell. 姚天顺 等, 1995, 《自然语言理解》，清华大学出版社，第四章，pp50-81, （语义网络） 张普,1991,《信息处理用现代汉语语义分析的理论与方法》，载《中文信息学报》1991年第3期。 "},{"title":"Language and history","date":"2017-06-01T15:00:00.000Z","url":"/2017/06/01/2017-06-01-Linguistics-and-history/","tags":[["Linguistics","/tags/Linguistics/"]],"categories":[["Linguistics","/categories/Linguistics/"]],"content":"语言是一个很神奇的存在，几乎每个正常的成年人都知道什么是语言，都知道怎么使用语言。在使用的时候，大部分人都会从一个人使用语言的过程中产生一些基本的判断。比如感觉某一个人很会说话，某一个人说话总是很难听，或者某个人说话的时候很有幽默感，某个人说的话很消极之类。 然而很多时候，我们并不会深入去想这些问题，为什么我们在进行语言互动的过程中，会产生这样或者那样的判断。究竟是语言中的哪些部分决定了我们产生了某种想法呢？这些问题并不像看上去那么容易找到答案。生理上看，语言仅仅是人类的声带在一定方式的震动下发出的特定频率的声波，然后传到我们的耳朵中，大脑接收到这些声波。然而，在一个人发出”Hello World” 这样一个声音，并传到我的耳朵中之后，我并不知道，我的大脑做了哪些工作，并使我将声音的信息和“你好”，“世界”这样的概念联系到了一起。我很清楚的知道，我理解这个声音表达的信息，但是严格来说我并不能证明我真的理解了这样一个信息。我可以说出这两个英文词的中文意思，我也可以说出”hello world” 这两个词是很多程序员在开始学习编程语言时使用的打印输出文本。甚至我可以大开脑洞，当这个声音是某个具有人工智能的电脑程序发出的对这个世界的问候。不过归根到底我并不能证明，我理解了这个语言片段。理解这个词的概念是模糊的，这个问题也最终沦为了一个形而上的哲学问题。 人类文明史即语言史从整个地球的历史来看，人类本身的文明发展史，是一个极其小的时间片段。而人类对于自身文明史的考证，毫无疑问是建立在对古代人类各种历史遗迹信息上的。在这些信息当中，古代人留下的语言和文字遗迹，则毫无疑问是对人类高度的文明最强有力的证明。 暂且以四大文明古国为例，各自的发展过程中，都分别孕育除了自己的语言和文字系统。正是有了这些文字信息才得以将各自的历史和故事记录下来，并传承下来。就在这些语言和文字的传承过程中，语言本身也随着各个文明的发展不断的调整和进化，同时反过来推动人类文明的社会结构的升级。人与人之间沟通的效率越来越高，人类社会的分工合作越发的强大。 在这里可以以中国作为更为典型的例子，来对整个中华文明的发展历程进行一个简单的回顾。例如，甲骨文是中国汉字的发展过程中，可考的最早出现的成系统的较为成熟的汉字。甲骨文，又称“契文”、“甲骨卜辞”、殷墟文字或“龟甲兽骨文”。甲骨文记录和反映了商朝的政治和经济情况，主要指中国商朝后期（前14～前11世纪）王室用于占卜吉凶记事而在龟甲或兽骨上契刻的文字，内容一般是占卜所问之事或者是所得结果。殷商灭亡周朝兴起之后，甲骨文还使用了一段时期，是研究商周时期社会历史的重要资料。紧接着甲骨文的金文和石鼓文，在结构和系统上和甲骨文是类似的，但是其文字本身进化的更为标准和严谨。这个语言文字体系一直持续到春秋战国时期，在古汉语研究领域将这些文字统称为“大篆”。 秦朝统一六国之后，才将之前混乱的语言文字体系进行了统一。经过秦国丞相李斯的整理，汉字被统一为一种通行书体。这种书体被称之为小篆，同时也可以叫做秦篆。这是中国古代文明发展历史过程中一个里程碑式的标志。第一次，在中国的大地上，原先所有国家的人民现在终于可以用通用的汉字系统进行表达和交流。这一点极大的促进了中华文明本身的发展，直接促进了中国古代经济和文化的告诉发展。这个时期，中国古代文明在制度的作用下，利用语言的统一直接推动了中国文明的发展。 随后中国汉字迎来了发展历史中最重要的一个事件，隶书出现了。相传隶书的创始人是程邈，因他得罪了秦始皇，下了监狱，在狱中用了十年的工夫，整理出一套应用简便的新字体，被后人称为隶书。在小篆统一了文字形式之后，隶书将原来的复杂统一形式进行了高度的简化，这个简单化的思想则是现代汉字发展过程中最重要的一个思想。到了汉代，隶书逐渐成熟，占据了主要地位。 随着历史的继续推进，终于在隶书的基础上，中国文明演化出了楷书这一一直延续到今天的正体字。楷书是我国封建社会南北魏到晋唐最为流行的一种书体。楷书也叫正楷、真书、正书。从程邈创立的隶书逐渐演变而来，更趋简化，横平竖直。 而中国大陆的汉字系统，之后还经历过一次大的简化运动，这里的核心思路没有变。都是在原有繁体字的基础上，将一些复杂的比划进行简写，简化了整个汉字系统。当然世界上现在还有一些地区在使用繁体字，但是大体的方向上是没有问题的，都是对原有书写方式的简化。 在中国文明发展的这个例子中，可以很明显的看出：随着中国古代文明程度越高，汉字的书写方式一直在不断的简化，在这个过程中简化后的语言，每一次都对整个社会的经济和文化发展起到了极大的推动作用。文明的进步和汉字的简化是一个正相关的过程，具体相互影响的细节可能并不是特别清楚，但是在大趋势上能明显的看出，文明的发展史就是该文明语言文字的发展史。 人的一生即语言理解和运用的一生在我们每一个出生的时候，都是没有使用语言能力的，但是最后我们都长成了能熟练使用语言的成人。这其实又是一个不清不楚的过程。虽然每个人都能学会自己的母语，可是，在这背后，我们对于自己学习语言这一个事情本身并没有足够的了解。我们自己的大脑在整个语言学习的过程中到底起到了什么作用，以及具体是如何发挥作用的，我们自己并没有找到明确的答案。 小时候，我们咋呼着要吃饭饭，在我们说出这样的语言时，大家并不会觉得奇怪。到了我们进入学校，渐渐的长大的时候，如果还说这样说，那估计打死我们也说不出来。然后，我们在经历了社会的种种，到了而立之年的时候，有幸遇到一个难得的知己，两杯浊酒相视而笑，一句“请”，表达出的又是那么丰满的情绪和感概。 这里我们其实有一个特别具有代表性的例子，能让我们清晰的看到我们自己的成长和语言理解和使用能力的关系。不知你是否还能想起，在小学时，语文老师给我们布置的作业。那些一百字，两百字的日记，还有作文作业？那时的我们一件件的记录着我们自己生活的流水账，试着用清晰的，完整连贯的语言表达出来，然而我相信在大部分人的记忆中，这也并不是一件非常容易的事情。 到了初中，我们开始认真的书写那500字的文章，终于我们可以有实力应付这几百字的表达任务。这个时候，我们再重新拾起小学时我们自己写下的文字，也许我们会被自己当年拙略的表述感动的不好意思。但是这个时候，已经基本能够理解成人世界中的大部分词汇和表达含义了。 然后，到了高中的时候，这语言的表述能力甚至第一次在我们人生的过程中起到了决定性的作用。那高考语文最后作文的几十分，也许就是北京大学和一所普通211之间的差距，又或者一本和二本之间的差距。而且我相信到了这个阶段，语言本身对于我们来说已经不仅仅是用来说事的工具了。也许你已经送出了你人生的第一封情书，也许你在日记中写下了你的第一次失恋，甚至你在课本的角落写下了那个你永远不会忘记的名字。 讨论人生是一个极其复杂的命题，但是这个过程中语言毫无疑问成为了一个人与世界沟通的主要方式。有诗意，亦有失意，有了念想，也有了立场。 句读之不知，惑之不解“句读之不知”这句话来自于韩愈的《师说》，韩愈这篇文章详细的阐述了老师在一个人的学习生活中的重要性。这句话中的“句读”一词是表示在古汉语学习中的一个很重要的环节，表示句子中不同词语之间的停顿。由于在古代，汉字系统中没有标点符号的存在，因此在学习前人文章的时候，断句就变得非常重要。有一些时候，断句方式的不同直接导致了一个句子含义解读成了完全不同的方向。当时的情况系下，学习语言，至少是书面语言是一个十分繁重的工作。尽管文字已经统一，而且单个文字的书写经过了系统的简化过程，但是由于句读这个问题的存在，所以需要大量的阅读和背诵，以便于通过经验的积累逐步培养出自己进行句读的能力。 其实不仅仅是古代，即使是在今天，在汉语语文的学习过程中，似乎很大程度上和古代汉语的学习方式没有什么改变。通过大量的阅读信息的积累，以培养出一种近乎直觉的对语言本身的感知，并在这个过程中掌握其用法。从本质上来说，无论我们最后是否真的能够按照预期的方式，使用语言进行表达和沟通，很多时候我们对语言本身并没有什么了解。也许是因为中国的文字语义的模糊性，很多时候，导致我们自己都不知道我们自己究竟是不是真的理解了一个语言片段。相当一大部分的人，在说“我懂了”的时候，甚至仅仅只是在表述，这个语言片段中没有我不认识的字罢了。 考虑到中国在民国之前，几乎没有科学的概念，更不用说成熟的科学研究方法。我们可以认为，中国语言学的研究，是在进入民国时期之后的事情。这个时期，中国最早的一批语言学研究者，通过借鉴西方的科学研究方法，开始对汉语进行研究。不过因为特殊的历史时期，这个时期的语言学家通常也是史学家，哲学家。那是一个大师辈出的时代。 这里我想介绍两个公认的大师，同时也是那个时代几乎开天辟地式的大家————陈寅恪和赵元任。下面是摘自百度百科的对两位大师的简介： 陈寅恪（1890.7.3—1969.10.7），字鹤寿，江西修水人。中国现代最负盛名的集历史学家、古典文学研究家、语言学家、诗人于一身的百年难见的人物，与叶企孙、潘光旦、梅贻琦一起被列为清华大学百年历史上四大哲人，与吕思勉、陈垣、钱穆并称为“前辈史学四大家”。 赵元任（1892.11.3—1982.2.24），汉族，字宣仲，又字宜重 ，原籍江苏武进（今常州）。清朝著名诗人赵翼（瓯北）后人。光绪十八年（1892年）生于天津。现代著名学者、语言学家、音乐家。赵元任是中国现代语言学先驱，被誉为“中国现代语言学之父”，同时也是中国现代音乐学之先驱，“中国科学社”的创始人之一。 陈寅恪的成就自不用说，其学术出成就远远不只是在语言学上，其学术工作在中国近代发展上有着极其重要的影响。其父亲陈三立更是在那个年代设立私人学堂，完全采用现代化的教育方式，学习语言，历史，音乐，外语，数学等学科。在这样的环境影响下，陈寅恪很早就有了西方现代化科学研究的基础，尤其是数学这类科学理工类的课程。 后来其在国外留学18年，是罕见的语言天才。他曾在美国哈佛大学随兰曼教授学习梵文、巴利文两年，后来又回到柏林大学研究院研究梵文及其他东方古文字学四年。回国之后，他又在北京和汉学家钢和泰教授继续研究梵文四五年。陈寅恪一生中通晓的语言有二三十种之多。英文、法文、德文、俄文、日文自不必说，他还精通梵文、巴利文、满文、蒙文、突厥文、西夏文、中古波斯文，还有拉丁文、马扎尔文等等。 赵元任的成就则更加集中在语言学研究和音律相关的工作。赵元任本科在康奈尔大学学习数学，选修物理和音乐，后来在哈佛大学获得哲学博士学位。1925年赵元任回清华大学教授数学、物理学、中国音韵学、普通语言学、中国现代方言、中国乐谱乐调和西洋音乐欣赏等课程。他与梁启超、王国维、陈寅恪一起被称为清华“四大导师”。 虽然出身不同，但是能明显看出这两位大师在背景上很多相似的地方。两人都留学多年，都有西方现代科学基础素养，都对语言有着近乎万中无一的天才和热情。而作为中国近代语言学之父的赵元任更是学习数学专业出身，物理哲学博士。由此便可以看出，真正的研究不仅仅是能够记住和背下更多的古籍和史料，而是建立在现代科学基础方法上进行的探索。而且据说当年还是近代大哲学家，数学家罗素的建议，让赵元任下定了一生将语言学研究作为事业的决心。 所有的科学研究都应该是建立在数理逻辑的基础之上，语言研究也不例外。语言本身就是可以被研究的一种人类特有的社会现象。 而我很惭愧，在大学之后，学习了中西方近代哲学，同时有机会学习北大胡壮麟教授的《语言学教程》之后，才打开了异世界的大门。后来我知道了乔姆斯基，知道了0-3型文法。知道了数学的集合论和语言学在这里达到了形式上的统一。知道了计算语言学和计算机的强大。随后更是追本溯源知道了图灵，知道了图灵和丘奇关于可计算数的工作。 原来真正是“文理不分家”！唯有对真理的追求永恒不变。 牛顿的苹果整个人类文明的发展过程在相当长的一段时间内是极其缓慢的，不同朝代更迭，但是社会形态却并没有什么本质上的变化。但是牛顿经典力学的出现，打破了一切按部就班，同时也让整个西方世界在极短的时间内掌握了足够的科学技术和工程能力。而这最直接的后果就是中国近代所面临的局面，一个不缺资源不缺吃穿的泱泱大国忽然被西方的蛮夷反超，并被摁在地上一顿暴揍。 关于牛顿的苹果对于整个科学的发展史究竟有多大的影响力，我们可以从跨度更大的人类科学发展史来进行观察： 第一个大的阶段，可以从人类发展出系统的语言和文字一直到16世纪之前。这个阶段，整个世界的科学发展水平是差距不大的。每一个独立的文明，都发展出了自己的计数系统，甚至是天文，初等几何，物理，哲学系统。在这一个阶段，对于科学的概念，更多的是一种哲学上的思考，一种对现象背后原理的反思。工程则更多的是一门手艺，一种基于前人工匠经验的积累和实现。这个时期，无论从西方的历史还是中国历史，都能找到很多碎片式的对一些具体问题的进行研究的著作。总体的来说，这个阶段虽然也出现了无数的大师，但是并没有形成成熟有效的科学研究体系。 第二个阶段，几乎就是以牛顿一个人之力打开的局面。当然，严格来说，同时期的莱布尼茨等大师也起到了极其重要的作用。1643年的1月，一个早产儿出生在英国英格兰林肯郡的向下，但是谁也没想到这样一个小时候若不惊风，看上去平淡无奇的一个早产儿却成为了西方科学发展史上最重要的人物。牛顿一生中最重要的工作有三个极其突出：1.物理学上提出了万有引力和经典的力学三定律，提出了牛顿运动定律，光学上发现了光的光谱等。2.数学上，和莱布尼茨分别独立的发明了数学上最重要的工具————微积分。3.经济学上，提出了基于金本位的世界经济体系。正是这些开拓性的工作，将人类文明发展过程中人类抽象的模型和实际的物理现实紧紧地联系到了一起。也正式由于这些理论工具的出现，人类在对自然现象进行探索的时候，有了强大的工具。这些工具直接使人类摆脱了在研究某些问题时，高度依赖于某些特定职业前人的情况。西方开始在这些思潮的影响下，逐步的发展出成熟的科学研究体系。也正式在这一段时期，中国似乎被西方残忍的甩到了后面。这一时期，中国正处在明清该朝换代的时期，也正式从这里开始，一切都似乎都不一样了。 第三个阶段，也几乎同样是由另一个划时代的大师以一人之力引领的。19世纪下半叶，康托尔提出了著名的集合论。之前科学领域的各个分支和问题，在集合论的系统下得到了高度的统一。现代数学本身的严格基础，就是建立在集合论的理论框架之上的。随着历史的推进和发展，集合论成为了几乎整个科学的基础，究其原因我个人认为就是集合论本质上就是人类抽象思维的实际体现。甚至于集合论目前存在的问题，也似乎是人类自身思维能力的限制。就像1931年，著名的逻辑学家哥德尔提出的哥德尔不完备定理，直接使数学的基础研究发生了几乎是摧枯拉朽式的变化。 从上面人类科学发展的三个重要阶段就可以看出，无论是第二阶段还是第三阶段都对人类整个科学发展史是转折性的重要关键点。尤其是第二阶段的牛顿，更是如有神助一般，给整个人类带来了神之视角。正是带来的神之视角，使我们有了基础的理论工具，可以来直接对我们周围的一切自然现象和社会现象进行研究，包括对语言的研究。当然，在对语言细分领域的研究历程中，也有这样的开天辟地的大师，甚至而这些大师给我们的生活带来的发展速度之快，远远超过我们自己的想想。其中这个领域研究最重要的一些人，现在依然和我活在同一个世界。我发自内心的感到兴奋，能够见证这一切，甚至参与这个过程当中。 上面所描述的整个科学发展的历程和产生的人类智慧的成果，是现代一切科学研究的基础。在这这样的一个背景下，我们就可以很清晰的看到：在研究语言现象的时候，也是建立在这个基础之上的。如果仅仅只是建立在个人经验上的模棱两可的“我认为”，则完全和第一阶段的时候，没有什么本质上的区别。 说到这里，我们可以大概的看出，在面对这我们人类自己的语言现象时，我们应该采取的态度了。高度成熟的抽象化语言系统本身是一种在人类社会存在特有现象，该现象的出现与人类历史文明的延续，人类生理上大脑结构，人类社会的教育系统，人类认知的本质有着重要的联系。而我个人认为，对于语言现象的深入研究，本质上就是对人类智能的研究，就是对“我是谁”这样一个这哲学终极之问的探索过程。甚至有可能超越我们人类本身，探索智本质的一个过程。 上帝说，要有光，于是便有了光。我们现在是不是也要成为造物主了呢？"},{"title":"AI that has Values","date":"2016-10-14T15:00:00.000Z","url":"/2016/10/14/2016-10-15-Translation-AI-values/","tags":[["Translation","/tags/Translation/"]],"categories":[["Translation","/categories/Translation/"]],"content":"让人工智能完美运行：我们怎么给人工智能系统赋予价值观？ Keeping AI Well Behaved:How Do We Engineer An Artificial System That Has Values? In Brief 摘要： It is estimated that some 10 million self-driving cars will be on the road by the close of 2020, which raises questions about how an AI will respond in lethal situations.预计到2020年为止，全球将会有超过100万辆人工智能驱动的无人车上路，同时也引出了人工智能在发生致命事故时会怎样反应的问题。 In this exclusive interview, FLI researcher David Parkes discusses his work to engineer a value-aligned AI system.在这篇专访中，FLI的研究员大卫.帕吉斯讨论了他构建带有价值观的人工智能系统的一些工作。 Teaching AI to make Decisions教人工智能进行讨论 Imagine you’re sitting in a self-driving car that’s about to make a left turn into on-coming traffic. One small AI system in the car will be responsible for making the vehicle turn, one system might speed it up or hit the brakes, other systems will have sensors that detect obstacles, and yet another system may be in communication with other vehicles on the road. Each system has its own goals — starting or stopping, turning or traveling straight, recognizing potential problems, etc. — but they also have to all work together toward one common goal: turning into traffic without causing an accident.想象一下你正坐在一辆自我驱动的无人车中，即将左转面对迎面而来的车流。车内的人工智能系统将会负责控制转向的过程，一个系统也许会加速或者是刹车，另一些系统将会通过传感器探测障碍物，还有一些系统也许会和路上的其他车辆进行通信。每一个系统都有其自己的目标 — 启动或者停止，转向或者直接行驶，分辨出潜在的问题等等。— 但是他们都需要被整合到一起朝向同一个目标：在不发生事故的情况下转换为正常的交通。 Harvard professor and Future of Life researcher, David Parkes, is trying to solve just this type of problem. Parkes told FLI, “The particular question I’m asking is: If we have a system of AIs, how can we construct rewards for individual AIs, such that the combined system is well behaved?”哈佛教授，未来生活研究员，大卫.帕吉斯，正在试图解决这一类型的问题。帕吉斯告诉FLI，“实际上我正要解决的问题是：如果我们有一个人工智能系统，我们如何才能为其中每一个独立的系统构建奖励模式，以便于整合的系统可以完美的运行？” Essentially, an AI within a system of AIs—like that in the car example above—needs to learn how to meet its own objective, as well as how to compromise so that it’s actions will help satisfy the group objective. On top of that, the system of AIs needs to consider the preferences of society. For example, it needs to determine if the safety of the passenger in the car or a pedestrian in the crosswalk is a higher priority than turning left.尤其是，一个人包含很多子系统的工智能系统 — 比如上面无人车的例子中说的 — 需要学会怎样才能完成自己的目标，同时还要学会妥协以便于其子系统的行为能对完成宏观的目标提供帮助。在这些问题之上，包含多个子系统的人工智能需要将社会的价值偏好考虑进来。比如，当其需要决定在转弯的过程中究竟是车内乘客的安全更为重要还是人行道上行人的安全更重要。 Because environments like a busy street are so complicated, an engineer can’t just program an AI to act in some way to always achieve its objectives. AIs need to learn proper behavior based on a rewards system. “Each AI has a reward for its action and the action of the other AI,” Parkes explained. With the world constantly changing, the rewards have to evolve, and the AIs need to keep up not only with how their own goals change, but also with the evolving objectives of the system as a whole.因为像忙碌的街道这样的环境太复杂，工程师无法通过简单的编程告诉人工智能去如何通过某种特定的行为模式去反应，同时又总能完成其目标。人工智能需要通过基于奖励的系统去学习合适的行为。“每一个人工智能的行为都有自己的激励模式，同时其他系统也有。” Making an Evolving AI构建一个自我进化的智能系统 The idea of a rewards-based learning system is something most people can likely relate to. Who doesn’t remember the excitement of a gold star or a smiley face on a test? And any dog owner has experienced how much more likely their pet is to perform a trick when it realizes it will get a treat. A reward for an AI is similar.基于奖励的学习系统的想法其实也和大多数人的经历相关。谁不记得在考试之后得到金色小星星或者一个笑脸时的兴奋呢？而且每一个有宠物狗的主人都有这样的经验：当宠物狗发现某种行为可以获得奖励的时候，它们是多么愿意执行某种特定的行为。对于人工智能的奖励也是类似的道理。 A technique often used in designing artificial intelligence is reinforcement learning. With reinforcement learning, when the AI takes some action, it receives either positive or negative feedback. And it then tries to optimize its actions to receive more positive rewards. However, the reward can’t just be programmed into the AI. The AI has to interact with its environment to learn which actions will be considered good, bad, or neutral. Again, the idea is similar to a dog learning that tricks can earn it treats or praise, but misbehaving could result in punishment.在设计人工智能系统的时候有一种常用的技术 — 强化学习。在强化学习的作用下，当人工智能采取行动的时候，其会获得积极的或者消极的反馈。而且其随后就会通过最优化其行为去获得更为积极的奖励。然而，这奖励不能直接被编程到AI的系统中。这些人工智能必须要和周围的环境进行互动，去学习哪些行为被认为是好的，坏的或者中性的。同样，这个想法和训练宠物狗的过程类似：执行某种行为获得奖励或者赞美，做错了则可能会引发惩罚。 More than this, Parkes wants to understand how to distribute rewards to subcomponents (the individual AIs) in order to achieve good system-wide behavior. How often should there be positive (or negative) reinforcement, and in reaction to which types of actions?不仅如此，帕吉斯想要理解如何通过将奖励分发给子系统（独立的人工智能）以便于能够产生系统层面的合理行为。正向强化或者负向强化的频率要怎么控制，而且应该基于哪种行为去强化呢？ For example, if you were to play a video game without any points or lives or levels or other indicators of success or failure, you might run around the world killing or fighting aliens and monsters, and you might eventually beat the game, but you wouldn’t know which specific actions led you to win. Instead, games are designed to provide regular feedback and reinforcement so that you know when you make progress and what steps you need to take next. To train an AI, Parkes has to determine which smaller actions will merit feedback so that the AI can move toward a larger, overarching goal.例如，如果你正在玩一款视频游戏，但是没有分数，没有命数限制或者其他的成功或者失败的提示，你也许会满世界跑打外星人，杀怪玩，也许最后游戏被你通关了，但是你不知道具体什么样的行为可以让你获得胜利。相反，游戏通常被设计成频繁的进行反馈和强化，这样你可以知道，你正在获得一定的进展而且你知道下一步你要做什么。为了训练一个人工智能系统，帕吉斯必须决定哪些小的行为会产生反馈以至于人工智能能够朝向更加宏观的目标。 But this is all for just one AI. How do these techniques apply to two or more AIs?但是这只是对于单个人工智能而言。这些技术怎样被应用于两个或者更多的人工智能混合的系统呢？ Gaming the System用博弈训练整个系统 Much of Parkes’ work involves game theory. Game theory helps researchers understand what types of rewards will elicit collaboration among otherwise self-interested players, or in this case, rational AIs. Once an AI figures out how to maximize its own reward, what will entice it to act in accordance with another AI?帕吉斯很大一部分工作包含博弈论。博弈论能帮助研究者理解哪些奖励的类型会引出“自私的”局中人的合作行为，在当前的情况中，是理性的人工智能系统。一旦人工智能搞清楚如何去让自身的奖励最大化，什么会影响其系统和其他人工智能系统的协同呢？ To answer this question, Parkes turns to an economic theory called mechanism design.为了解答这些问题，帕吉斯转向了对一种叫机制设计的的经济学理论的研究。 Mechanism design theory is a Nobel-prize winning theory that allows researchers to determine how a system with multiple parts can achieve an overarching goal. It is a kind of “inverse game theory.” How can rules of interaction – ways to distribute rewards, for instance – be designed so individual AIs will act in favor of system-wide and societal preferences? Among other things, mechanism design theory has been applied to problems in auctions, e-commerce, regulations, environmental policy, and now, artificial intelligence.机制设计理论是一个获得过诺奖的理论，这个理论允许研究者去决定一个多部分组成的系统去如何获取宏观的目标。这是一种“反向博弈论”。人工智能系统之间的互动规则 — 在这里是发放奖励的方法或者途径 — 如何被设计用来推动系统层面或者社会层面的偏好？在这些问题中，机制设计理论已经被应用到很多实际的领域中，电子商务，法律法规，环境政策，现在，人工智能。 The difference between Parkes’ work with AIs and mechanism design theory is that the latter requires some sort of mechanism or manager overseeing the entire system. In the case of an automated car or a drone, the AIs within have to work together to achieve group goals, without a mechanism making final decisions. As the environment changes, the external rewards will change. And as the AIs within the system realize they want to make some sort of change to maximize their rewards, they’ll have to communicate with each other, shifting the goals for the entire autonomous system.帕吉斯人工智能系统的工作和机制设计理论的不同在于后者要求某种机制或者管理者来监督整个系统。而在自动驾驶汽车或者无人车的例子中，其中包含的人工智能系统需要一起协作区完成整个团体的目标，并不需要一个机制去做最终的决策。当环境改变的时候，内部的奖励也会改变。并且当这些人工智能系统意识到他们需要做一些改变，以减少其奖励，他们会相互进行通信，将目标和整个自动驾驶系统做整合协调。 Parkes summarized his work for FLI, saying, “The work that I’m doing as part of the FLI grant program is all about aligning incentives so that when autonomous AIs decide how to act, they act in a way that’s not only good for the AI system, but also good for society more broadly.”帕吉斯对FLI总结了其工作，他说，“我正在做的事情，作为FLI整个项目的一部分，其实就是协调激励以便于自动的人工智能系统可以决定如何去行动，而且不仅仅是基于为了自身系统的利益，更要基于广义上社会的利益。” Parkes is also involved with the One Hundred Year Study on Artificial Intelligence, and he explained his “research with FLI has informed a broader perspective on thinking about the role that AI can play in an urban context in the near future.” As he considers the future, he asks, “What can we see, for example, from the early trajectory of research and development on autonomous vehicles and robots in the home, about where the hard problems will be in regard to the engineering of value-aligned systems?”帕吉斯同时也参与了“人工智能百年研究”项目，他这样解释自己的工作“在FLI的研究为自己提供了一个更加宽广的角度，以便思考在未来的城市生活中人工智能可以扮演怎样的角色。”他同时也对未来有所顾虑，他问道，“我能可以看到什么，比如，早期的研究路径和自动驾驶汽车以及家庭机器人，包括在有价值倾向的人工智能系统工程中，困难的问题又会在什么地方出现呢？” "},{"title":"Sorting algorithms","date":"2016-10-08T15:00:00.000Z","url":"/2016/10/08/2016-10-08-Solution-quicksort/","tags":[["Sorting QuickSort BubbleSort","/tags/Sorting-QuickSort-BubbleSort/"]],"categories":[["Algorithm","/categories/Algorithm/"]],"content":"题目：请手写出两种以上排序算法，并分析不同情况下复杂度的变化情况（语言不限）。 思考：算法分析领域，排序算法应该算是最基础的，入门就会接触到的算法。但是在这看似简单的排序上，却体现了算法分析最精髓的思想。在排序算法的历史发展过程当中，有无数大神发明了N多种排序算法，也有给CS的学生提供了大量论文素材。我在这里仅仅选择一些简单的典型进行实现，目的是为了帮助自己进一步理解其中的核心思想。 最常见的排序莫过于冒泡，快排，归并了。具体的实现代码亦有参考一些前辈。欢迎大家一起交流，学习。 C#排序算法实现： JAVA实现上述三种排序 python实现以上三种算法"},{"title":"Solution of Equilibrium Index Problem in O(N)","date":"2016-10-07T15:00:00.000Z","url":"/2016/10/07/2016-10-07-Solution-equilibrium-index/","tags":[["Equilibrium Index O(N)","/tags/Equilibrium-Index-O-N/"]],"categories":[["Algorithm","/categories/Algorithm/"]],"content":"In this blog, I gave the solution of Equilibrium Index Problem (O(N)) in Three Languages. The first one is writen in java The second one is writen in python The third one is writen in c#"}]